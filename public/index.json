[{"authors":null,"categories":null,"content":"Oluwafemi Oyedele is a data analyst and a R studio instructor. I earned my Master degree in Agrometeorology from the University of Ibadan, Nigeria, under the supervision of Prof. K. O. Oluwasemire, Dr. Stefan Hauser, Dr. Moses Ogunlade and Mr. Deo Gratias Hougni. My research focuses on soil fertility, agrometeorology, simulation modeling, agricultural field experimentation and machine learning. During the past few years, I discovered the power of the statistical programming language R. Since then I have turned into a big enthusiast, using the software almost every day for work and many private programming projects. I love analyzing data and making beautiful charts with R and I have mentored countless students and scientists in everything from experimental design to using R for creating figures for their publication. My preferred tool for data analysis has been R but I also work with Python. I am also very passionate about teaching and sharing knowledge, so I give workshops, talk at conferences or meetups, write blog posts and organize R meetups. Sponsor me on GitHub\n","date":1593561600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1593561600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://statisticalinference.netlify.app/author/oluwafemi-oyedele/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/oluwafemi-oyedele/","section":"authors","summary":"Oluwafemi Oyedele is a data analyst and a R studio instructor. I earned my Master degree in Agrometeorology from the University of Ibadan, Nigeria, under the supervision of Prof. K. O.","tags":null,"title":"Oluwafemi Oyedele","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e87efdfe209d90ea3c6332a7cbd9d08b","permalink":"https://statisticalinference.netlify.app/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"summary: An example of using the tidyverse to solve real world problem.\n\r Table of Contents\r What you will learn Program overview Program overview Courses in this program Meet your instructor FAQs  \r\rWhat you will learn  Fundamental Tidyverse programming skills Data management using the tidyverse Gain experience with the Tidyverse, including data visualization with ggplot2 and data wrangling with dplyr, lubridate and forcat  Program overview The demand for skilled data science practitioners is rapidly growing. This course will give you the necessary skills required to solve real world problem.\n Fundamentals of the Tidyverse Statistical concepts and how to apply them in practice Gain experience with the Data management workflow in R, including data visualization with ggplot2 and data wrangling with dplyr  Program overview The demand for skilled data science practitioners is rapidly growing. This course will expose you to how to use the tidyverse to solve real world problem.\nCourses in this program \rR basics\rBuild a foundation in R. \r\rVisualization\rLearn how to visualize data with ggplot2. \r\rStatistics\rIntroduction to statistics for data science. \r\rMeet your instructor Oluwafemi Oyedele FAQs Are there prerequisites?\rThere are no prerequisites for the first course.\n How often do the courses run?\rContinuously, at your own pace.\n \rBegin the course\r\r\rh1.title {\rfont-size: 12px;\rcolor: Dark;\rtext-align: centre;\r}\r\rbody{\rtext-align: justify}\r\r\rBegin the course\r\r\r","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://statisticalinference.netlify.app/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"A 2-weeks workshop introducing the Tidyverse. In this workshop, you will learn how to read data with readr, readxl and haven, how to tidy and transform data with Tidyr, dplyr, lubridate and forcats, and how to produce high quality visualization with ggplot2. You will also learn how to build model with broom and tidymodel and then how to produce report with rmarkdown.","tags":null,"title":"Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in R.\n\r 1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples?\rLists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is R case-sensitive?\rYes\n h1.title {\rfont-size: 12px;\rcolor: Dark;\rtext-align: centre;\r}\r\rbody{\rtext-align: justify}\r","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://statisticalinference.netlify.app/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in R.\n","tags":null,"title":"R basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with ggplot2.\n\r 1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful?\rLorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write ggplot2 code to render a bar chart\rdata(mtcars)\rlibrary(ggplot2)\rggplot(mtcars,aes(mpg,hp))+geom_point()\r  h1.title {\rfont-size: 12px;\rcolor: Dark;\rtext-align: centre;\r}\r\rbody{\rtext-align: justify}\r","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://statisticalinference.netlify.app/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with ggplot2.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n\r 1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n\rThe parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.\r\r\rQuiz What is the parameter $\\mu$?\rThe parameter $\\mu$ is the mean or expectation of the distribution.\n h1.title {\rfont-size: 12px;\rcolor: Dark;\rtext-align: centre;\r}\r\rbody{\rtext-align: justify}\r","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://statisticalinference.netlify.app/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":["Functional Programming"],"content":"\rIn this post I’m going to show some cool feature of Purrr. Purrr is an R package for functional programming. I have always been fascinated by functional programming. I first heard about it while I was reading Hadley R for Data Science book. With this approach, not only it makes our code more succinct and clear, but more expressive. Without further ado lets dive straight in. For this exercise, I’m using mtcars data set.\nLets load some libraries like tidyverse, broom and tidytext.\rSuppose if we want to run regression model on 3 sets of data sets grouped by certain feature (cyl in our case), we need to regress data set separately.\n\rRegression for dataset with cyl == 8\rcyl8 \u0026lt;- mtcars |\u0026gt; filter(cyl == 8)\rsummary(lm(mpg~wt, data=cyl8))\r## ## Call:\r## lm(formula = mpg ~ wt, data = cyl8)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -2.1491 -1.4664 -0.8458 1.5711 3.7619 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 23.8680 3.0055 7.942 4.05e-06 ***\r## wt -2.1924 0.7392 -2.966 0.0118 * ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 2.024 on 12 degrees of freedom\r## Multiple R-squared: 0.423, Adjusted R-squared: 0.3749 ## F-statistic: 8.796 on 1 and 12 DF, p-value: 0.01179\rSo we have to do like this for each cyl. One thing you might have noticed is that we have separate results. Imagine doing some analysis on bigger data set and having to keep track of each result separately. It would be a nightmare. You might say we can achieve by using loops but they have their own disadvantages. We can achieve this easily by use of Purrr. Another thing which is crucial is use of ‘tibble’. A tibble is similar to traditional data frame but much more efficient. I like mostly of the fact that it can store ‘list columns’. Let me show this.\nnested\u0026lt;-mtcars |\u0026gt; group_by(cyl) |\u0026gt; nest(cyl)\r## Warning: All elements of `...` must be named.\r## Did you want `data = cyl`?\rnested\r## # A tibble: 32 × 11\r## mpg disp hp drat wt qsec vs am gear carb data ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt; ## 1 21 160 110 3.9 2.62 16.5 0 1 4 4 \u0026lt;grouped_df\u0026gt;\r## 2 21 160 110 3.9 2.88 17.0 0 1 4 4 \u0026lt;grouped_df\u0026gt;\r## 3 22.8 108 93 3.85 2.32 18.6 1 1 4 1 \u0026lt;grouped_df\u0026gt;\r## 4 21.4 258 110 3.08 3.22 19.4 1 0 3 1 \u0026lt;grouped_df\u0026gt;\r## 5 18.7 360 175 3.15 3.44 17.0 0 0 3 2 \u0026lt;grouped_df\u0026gt;\r## 6 18.1 225 105 2.76 3.46 20.2 1 0 3 1 \u0026lt;grouped_df\u0026gt;\r## 7 14.3 360 245 3.21 3.57 15.8 0 0 3 4 \u0026lt;grouped_df\u0026gt;\r## 8 24.4 147. 62 3.69 3.19 20 1 0 4 2 \u0026lt;grouped_df\u0026gt;\r## 9 22.8 141. 95 3.92 3.15 22.9 1 0 4 2 \u0026lt;grouped_df\u0026gt;\r## 10 19.2 168. 123 3.92 3.44 18.3 1 0 4 4 \u0026lt;grouped_df\u0026gt;\r## # … with 22 more rows\rHere if you look at column data, it’s a list column. Each entry is a separate data frame. It’s like an entire Excel spreadsheet stored into that tiny cell. And this is made possible by using nest function from tidyr package. Lets see what’s inside one of them.\nnested$data[[1]]\r## # A tibble: 1 × 1\r## # Groups: cyl [1]\r## cyl\r## \u0026lt;dbl\u0026gt;\r## 1 6\rOk! as mentioned each entry of that list is a separate data frame.\nNow we can run regression on each entry of that list and store each model into another list column. For this we have to use map() function, which helps to iterate on each entry of that list and run regression.\nmodel_nested\u0026lt;-nested |\u0026gt; mutate(model = map(.x=data,.f =~lm(mpg~wt, data=.)))\rmodel_nested\r## # A tibble: 32 × 12\r## mpg disp hp drat wt qsec vs am gear carb data ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt; ## 1 21 160 110 3.9 2.62 16.5 0 1 4 4 \u0026lt;grouped_df\u0026gt;\r## 2 21 160 110 3.9 2.88 17.0 0 1 4 4 \u0026lt;grouped_df\u0026gt;\r## 3 22.8 108 93 3.85 2.32 18.6 1 1 4 1 \u0026lt;grouped_df\u0026gt;\r## 4 21.4 258 110 3.08 3.22 19.4 1 0 3 1 \u0026lt;grouped_df\u0026gt;\r## 5 18.7 360 175 3.15 3.44 17.0 0 0 3 2 \u0026lt;grouped_df\u0026gt;\r## 6 18.1 225 105 2.76 3.46 20.2 1 0 3 1 \u0026lt;grouped_df\u0026gt;\r## 7 14.3 360 245 3.21 3.57 15.8 0 0 3 4 \u0026lt;grouped_df\u0026gt;\r## 8 24.4 147. 62 3.69 3.19 20 1 0 4 2 \u0026lt;grouped_df\u0026gt;\r## 9 22.8 141. 95 3.92 3.15 22.9 1 0 4 2 \u0026lt;grouped_df\u0026gt;\r## 10 19.2 168. 123 3.92 3.44 18.3 1 0 4 4 \u0026lt;grouped_df\u0026gt;\r## # … with 22 more rows, and 1 more variable: model \u0026lt;list\u0026gt;\rHere, mutate is generic command to create a column called model. The meat of the operation starts with map function.\nPurrr have a different variant of map function. We have map(), map_int(), map_dbl(), map_chr(), map_lgl(). As you may have guessed, each one returns certain kind of data like map_int() returns Integers, map_dbl() returns Doubles, map() always returns list.\nSo in our above code, we can be certain that our result will be a list. You may be wondering what is (~)? It denotes an anonymous function, a function which is defined on a fly. so, lm(mpg~wt) denotes a linear regression being run with ‘mpg’ against wt. The (.) denotes the current data frame in that context. So what map has done is run 3 regression models and stored the respective results under model column. We can see what’s the first entry\nmodel_nested$model[[1]]\r## ## Call:\r## lm(formula = mpg ~ wt, data = .)\r## ## Coefficients:\r## (Intercept) wt ## 37.285 -5.344\rAs we see, it’s one of the models. We can get more info into it by running summary function.\nsummary(model_nested$model[[1]])\r## ## Call:\r## lm(formula = mpg ~ wt, data = .)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 \u0026lt; 2e-16 ***\r## wt -5.3445 0.5591 -9.559 1.29e-10 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 3.046 on 30 degrees of freedom\r## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10\rBut why do summary separately, if we know how to Purrr\nmodel_nested_summarised\u0026lt;-model_nested |\u0026gt; mutate(model_summary = map(.x = model, .f = ~summary(.) |\u0026gt;tidy()))\rmodel_nested_summarised\r## # A tibble: 32 × 13\r## mpg disp hp drat wt qsec vs am gear carb data ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt; ## 1 21 160 110 3.9 2.62 16.5 0 1 4 4 \u0026lt;grouped_df\u0026gt;\r## 2 21 160 110 3.9 2.88 17.0 0 1 4 4 \u0026lt;grouped_df\u0026gt;\r## 3 22.8 108 93 3.85 2.32 18.6 1 1 4 1 \u0026lt;grouped_df\u0026gt;\r## 4 21.4 258 110 3.08 3.22 19.4 1 0 3 1 \u0026lt;grouped_df\u0026gt;\r## 5 18.7 360 175 3.15 3.44 17.0 0 0 3 2 \u0026lt;grouped_df\u0026gt;\r## 6 18.1 225 105 2.76 3.46 20.2 1 0 3 1 \u0026lt;grouped_df\u0026gt;\r## 7 14.3 360 245 3.21 3.57 15.8 0 0 3 4 \u0026lt;grouped_df\u0026gt;\r## 8 24.4 147. 62 3.69 3.19 20 1 0 4 2 \u0026lt;grouped_df\u0026gt;\r## 9 22.8 141. 95 3.92 3.15 22.9 1 0 4 2 \u0026lt;grouped_df\u0026gt;\r## 10 19.2 168. 123 3.92 3.44 18.3 1 0 4 4 \u0026lt;grouped_df\u0026gt;\r## # … with 22 more rows, and 2 more variables: model \u0026lt;list\u0026gt;, model_summary \u0026lt;list\u0026gt;\rAgain, here we can see model_summary is a list column which stores summaries of each model. The first entry should be the same as above result.\nmodel_nested_summarised$model_summary[[1]]\r## # A tibble: 2 × 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 37.3 1.88 19.9 8.24e-19\r## 2 wt -5.34 0.559 -9.56 1.29e-10\rAgain, here we can see ‘model_summary’ is a list column which stores summaries of each model. The first entry should be the same as above result.\nmodel_nested_summarised$model_summary[[1]]\r## # A tibble: 2 × 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 37.3 1.88 19.9 8.24e-19\r## 2 wt -5.34 0.559 -9.56 1.29e-10\rWe can extract the necessary information from those list columns separately.\n#model_nested_summarised$model_summary[[1]][[\u0026quot;r.squared\u0026quot;]]\rBut lets go one step and get all r.squared values of each model and store them under separate column with the help of Purrr.\n#model_nested_summarised |\u0026gt; # mutate(rsqr = map_dbl(.x=model_summary,.f = ~.[[\u0026quot;r.squared\u0026quot;]]))\rHere we used map_dbl() as we want our result as Doubles not a list. Thus,we managed to get all the requested values under one data frame. This is neat.\nNow let’s try all of this using a package called broom. It presents all the result into a tidy format. I will compact all into one single block this time.\nall_in_one\u0026lt;- mtcars |\u0026gt; nest(-cyl) |\u0026gt; mutate(model = map(.x = data, .f = ~lm(mpg~wt, data = .)),\rtidied = map(model,glance)) |\u0026gt; unnest(tidied)\r## Warning: All elements of `...` must be named.\r## Did you want `data = -cyl`?\rall_in_one\r## # A tibble: 3 × 15\r## cyl data model r.squared adj.r.squared sigma statistic p.value df\r## \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 6 \u0026lt;tibble\u0026gt; \u0026lt;lm\u0026gt; 0.465 0.357 1.17 4.34 0.0918 1\r## 2 4 \u0026lt;tibble\u0026gt; \u0026lt;lm\u0026gt; 0.509 0.454 3.33 9.32 0.0137 1\r## 3 8 \u0026lt;tibble\u0026gt; \u0026lt;lm\u0026gt; 0.423 0.375 2.02 8.80 0.0118 1\r## # … with 6 more variables: logLik \u0026lt;dbl\u0026gt;, AIC \u0026lt;dbl\u0026gt;, BIC \u0026lt;dbl\u0026gt;, deviance \u0026lt;dbl\u0026gt;,\r## # df.residual \u0026lt;int\u0026gt;, nobs \u0026lt;int\u0026gt;\rWe use glance function to return all the output on each row-wise basis for each data frame. We can now use this output straight to plot some nice figures\n\r","date":1654128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654185561,"objectID":"a48eb358d712d4ab6a4f2f07a59da1db","permalink":"https://statisticalinference.netlify.app/post/function/","publishdate":"2022-06-02T00:00:00Z","relpermalink":"/post/function/","section":"post","summary":"In this post I’m going to show some cool feature of Purrr. Purrr is an R package for functional programming. I have always been fascinated by functional programming. I first heard about it while I was reading Hadley R for Data Science book.","tags":[],"title":"Functional Programming with Purrr","type":"post"},{"authors":[],"categories":null,"content":"","date":1648288816,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648288816,"objectID":"fd80010fa646a57eda32f3c3f55c0ace","permalink":"https://statisticalinference.netlify.app/talk/communicating-data-insights-with-visuals-using-ggplot2/","publishdate":"2022-03-13T00:00:00Z","relpermalink":"/talk/communicating-data-insights-with-visuals-using-ggplot2/","section":"event","summary":"I will speaking on communicating data insights with visuals using ggplot2 at Abuja R Users Group. Here I will be talking on all the layers of ggplot2 and the session is going to be very impactful.","tags":[],"title":"Communicating Data Insights with Visuals using ggplot2","type":"event"},{"authors":[],"categories":["R Shiny"],"content":"\r\rI am really excited to share with you the new R package radiant. Radiant’s goal is to provide access to the power of R for business analytics and data science using R and Shiny. Although Radiant’s web-interface can handle many data and analysis tasks.\rRadiant provides a bridge to programming in R(studio) by exporting the functions used for analysis. For example, you can run your analyses in Radiant and output the relevant function calls to R or Rmarkdown document and you can also send your codes directly to R Studio.\rAt the moments radiant is on CRAN and you can install it with the function below.\ninstall.packages(\u0026quot;radiant\u0026quot;)\rThen we have to load the library\nlibrary(radiant)\rThen we have to run the app but their is also a rstudio addin that we can use to run radiant.\nradiant()\rThis will then load a R shiny interface in our browser and the first thing that we need to do now is for us to import our data set and then we can do some data transformation and visualization.\rRadiant has several tabs i.e the model,multivariate and the last tab is for report, where we can knit our document to generate the output alongside with the codes. Radiant is a very good R package that is well documented, and you can check the github page of radiant here or you can check the vignette of radiant for more information here\n\r\r\r\r","date":1644364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644402280,"objectID":"1212b3c40831833ca4be564e7a25e644","permalink":"https://statisticalinference.netlify.app/post/r-programming-with-radiant-package/","publishdate":"2022-02-09T00:00:00Z","relpermalink":"/post/r-programming-with-radiant-package/","section":"post","summary":"I am really excited to share with you the new R package radiant. Radiant’s goal is to provide access to the power of R for business analytics and data science using R and Shiny.","tags":["R"],"title":"R Programming with Radiant Package","type":"post"},{"authors":[],"categories":null,"content":"h1.title {\rfont-size: 12px;\rcolor: Dark;\rtext-align: centre;\r}\r\rbody{\rtext-align: justify}\r","date":1637229616,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637229616,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://statisticalinference.netlify.app/talk/introduction-to-r-using-the-tidyverse-principles/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/talk/introduction-to-r-using-the-tidyverse-principles/","section":"event","summary":"I along side with Dr Ibnou Dieng (Head of Biometric Unit, IITA, Ibadan), Mr Moshood Bakare (PhD Student from Cornel University, USA) and Mr Kayode Fawobaje (Data Analyst, IITA) will be presenting the introduction to R using the tidyverse principles to research fellows at the International Institute of Tropical Agriculture (IITA), Ibadan, Nigeria. We will also explain to them the basic functions of data import, data management and then We will introduce them to data visualization using ggplot2. We will also cover multi-location trial using lme4 package in R, lmerTest, statgenSTA, statgenGxE and then we will summarize with multivariate analysis in R using Factoextra package.","tags":[],"title":"Introduction to R using the Tidyverse Principles","type":"event"},{"authors":[],"categories":["Functional Programming"],"content":"\r\rWhen you have a lot of variables and need to make multiple exploratory plots it’s usually worthwhile to automate the process in R instead of manually copying and pasting code for every plot, this thing happen to me alot but when I read Hadley Wickham Advance R book on functional programming, I knew that what I was doing was wrong. However, the coding approach needed to automate plots can look pretty daunting to a beginner R user. It can look so daunting, in fact, that it can appear easier to manually make the plots (like in Excel) rather than using R at all.\nUnfortunately making plots manually can backfire. The efficiency of using a software program you already know is quickly out-weighed by being unable to easily reproduce the plots when needed. I know I invariably have to re-make even exploratory plots, and it’d be a bummer if I had to remake them all manually rather than re-running some code.\nSo while I often assure students working under time constraints that it is perfectly OK to use software they already know rather than spending the time to learn how to do something in R, making many plots is a special case. To get them started I will provide students who need to automate plotting in R some example code (with explanation).\nThis post is based on an example that involves plotting bivariate relationships between many continuous variables.\nLoad R packages\rLoad the necessary library\rlibrary(ggplot2)\rlibrary(purrr)\rlibrary(cowplot)\rI’ll be plotting with ggplot2 and looping with purrr package. I’ll also be using package cowplot later to combine individual plots into one, but will use the package functions via my name space cowplot:: instead of loading the package.\n\rThe set-up\rHere I’m going to make an example dataset with 3 response (y) variables and 4 explanatory (x) variables for plotting.\nset.seed(16)\rdat = data.frame(elev = round( runif(20, 100, 500), 1),\rresp = round( runif(20, 0, 10), 1),\rgrad = round( runif(20, 0, 1), 2),\rslp = round( runif(20, 0, 35),1),\rlat = runif(20, 44.5, 45),\rlong = runif(20, 122.5, 123.1),\rnt = rpois(20, lambda = 25) )\rhead(dat)\r## elev resp grad slp lat long nt\r## 1 373.2 9.7 0.05 8.8 44.54626 122.8547 18\r## 2 197.6 8.1 0.42 33.3 44.79495 122.5471 26\r## 3 280.0 5.4 0.38 19.3 44.99027 122.9645 18\r## 4 191.8 4.3 0.07 29.6 44.95022 122.7290 19\r## 5 445.4 2.3 0.43 16.5 44.79784 122.9836 15\r## 6 224.5 6.5 0.78 4.1 44.96576 122.9836 21\rThe goal is to make scatterplots for every response variable vs every explanatory variable. I’ve deemed the first three variables in the data set to be the response variables (elev, resp, grad). The plan is to loop through the variables and make the desired plots. I’m going to use vectors of the variable names for this, one vector for the response variables and one for the explanatory variables. If all of your response or explanatory variables share some unique pattern in the variable names there are some clever ways to pull out the names with some of the select helper functions in dplyr::select(). My variable names are all unique. My options are to either write the vectors out manually or pull the names out by index. I’ll do the latter since the different types of variables are grouped together.\nresponse = names(dat)[1:3]\rexpl = names(dat)[4:7]\rWhen I know I’m going to be looping through character vectors I like to use named vectors. This helps me keep track of things in the output. The set_names() function in purrr is super handy for naming character vectors, since it can use the values of the vector as names (i.e., the vector will be named by itself). (I don’t recommend trying this with lists of data.frames like I have in the past, though, since it turns out that naming a data.frame with a data.frame isn’t so useful.)\nresponse = set_names(response)\rresponse\r## elev resp grad ## \u0026quot;elev\u0026quot; \u0026quot;resp\u0026quot; \u0026quot;grad\u0026quot;\rexpl = set_names(expl)\rexpl\r## slp lat long nt ## \u0026quot;slp\u0026quot; \u0026quot;lat\u0026quot; \u0026quot;long\u0026quot; \u0026quot;nt\u0026quot;\r\rCreate a plotting function\rSince I’m going to make a bunch of plots that will all have the same basic form, I will make a plotting function. I am going to make a function where only the x and y variables can vary (so are arguments to the function). Since I’m making a function to plot variables from a single data set I’m going to hard-code the data set into the function. If you have multiple data sets or you are making a function for use across projects you’ll probably want to add the data set as a function argument. My functions inputs are based on the variable names, so I need to pass strings into the ggplot2 functions. Strings cannot be used directly in aes(), but can be used with the .data pronoun. I’m making pretty basic graphs since these are exploratory plots, not publication-ready plots. I will make a scatterplot and add locally weighted regression (loess) lines via geom_smooth(). I use such lines with great caution, as it can be easy to get too attached any pattern the loess line shows.\nscatter_fun = function(x, y) {\rggplot(dat, aes(x = .data[[x]], y = .data[[y]])) +\rgeom_point() +\rgeom_smooth(method = \u0026quot;loess\u0026quot;,\rse = FALSE,\rcolor = \u0026quot;grey74\u0026quot;) + theme(\rpanel.grid.major = element_blank(),\rpanel.grid.minor = element_blank(),\rpanel.background = element_blank(),\raxis.line = element_line()\r)\r}\rIf using older versions of ggplot2 (and/or rlang), use the now deprecated aes_string() for working with strings.\nscatter_fun = function(x, y) {\rggplot(dat, aes_string(x = x, y = y)) +\rgeom_point() +\rgeom_smooth(method = \u0026quot;loess\u0026quot;,\rse = FALSE,\rcolor = \u0026quot;grey74\u0026quot;) + theme(\rpanel.grid.major = element_blank(),\rpanel.grid.minor = element_blank(),\rpanel.background = element_blank(),\raxis.line = element_line()\r)\r}\r\rHere’s an example of the function output, passing in x and y as strings.\rscatter_fun(x = \u0026quot;lat\u0026quot;, y = \u0026quot;elev\u0026quot;)\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r\rLooping through one vector of variables\rOne way to make all the plots I want is to loop through each explanatory variable for a fixed response variable. With this approach I would need a separate loop for each response variable.\n\rHere I will use the map function from purrr package for looping.\rI pass each explanatory variable to the first argument in scatter_fun() and I fix the second argument to “elev”. I use the formula coding in map() and so refer to the element of the explanatory vector via .x within scatter_fun().\nelev_plots = map(expl, ~scatter_fun(.x, \u0026quot;elev\u0026quot;) )\rThe output is a list of 4 plots (since there are 4 explanatory variables). You’ll notice that each element of the list has the variable name associated with it. This is why I used set_names() earlier, since this is convenient for printing the plots and, you’ll see later, is convenient when saving the plots in files with understandable names.\nelev_plots\r## $slp\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $lat\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $long\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $nt\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r\rLooping through both vectors\rFor only a few response variables we could easily copy and paste the code above, changing the hard-coded response variable each time. This process can get burdensome if there are a lot of response variables, though. Using a nested loop is another option which involve looping through both vectors of variables to make all the plots at once. Because we want a plot for each combination of variables, this is a job for a nested loop. This means one map() loop will be nested inside another. I will refer to the first map() loop as the outer loop and the second one as the inner loop. I’m going to run through the response variables in the outer loop and the explanatory variables in the inner loop. That way I can graph all of the explanatory variables for each response variable before moving on to the next response variable. This puts the output, a nested list, in a logical order. A nested loop involves more complicated code. It took some effort for me to wrap my head around how to refer to the list element from the outer loop within the inner loop when using the map() formula coding with the tilde (~). I found the answers/comments to this question on Stack Overflow to be helpful. Since my scatter plot function is so simple I ended up using formula coding for the outer loop and the function as-is in the inner loop. The inner list elements are fed to the first argument of scatter_fun() by default, which works out great since the first argument is the x variable and the inner loop loops through the explanatory variables. The .x then refers to the outer list elements (the response variable names), and is passed to the y argument of the function in the inner loop.\nall_plots = map(response,\r~ map(expl, scatter_fun, y = .x))\rTo be honest, I think it is fairly hard to follow what the above code is doing. And I’ve found it difficult to use such an approach if adding additional nested loops. At the end of the day, using anonymous functions involves more code but the results may be easier to understand. Well, as easy to follow as nested loops ever are. Here’s the same plots using anonymous functions in each map() loop.\nall_plots2 = map(response, function(resp) {\rmap(expl, function(expl) {\rscatter_fun(x = expl, y = resp)\r})\r})\rThe output is a list of lists. Each sublist contains all the plots for a single response variable. Because I set the names for both vectors of variable names, the inner and outer lists both have names. These names can be used to pull out individual plots. For example, if I want to see all the plots for the grad response variable I can print that sublist by name. (I’m going to display only two of four grad plots here to save space.)\nall_plots$grad[1:2]\r## $slp\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $lat\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rIf I want to print a single plot, I can first extract one of the sub-lists using an outer list name and then extract the individual plot via an inner list name.\nall_plots$grad$long\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rI find the names convenient, but you can also extract plots via position. Here’s the same graph, the third element of the third list.\nall_plots[[3]][[3]]\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r\rCombining variables for a single loop\rSince nested loops can be complicated, another option is to create all combinations of the two input vectors and then loop through those in a single loop. This can be done, for example, using tidyr::expand_grid(). The main thing I like about tidyr::expand_grid() over expand.grid() from base R is the order of the output. Both put the results into a data set (data.frame or tibble). Here’s how that would look, putting the response variables first to match the output order above. I’m using tidyr version 1.1.4 here.\nresp_expl = tidyr::expand_grid(response, expl)\rresp_expl\r## # A tibble: 12 x 2\r## response expl ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;\r## 1 elev slp ## 2 elev lat ## 3 elev long ## 4 elev nt ## 5 resp slp ## 6 resp lat ## 7 resp long ## 8 resp nt ## 9 grad slp ## 10 grad lat ## 11 grad long ## 12 grad nt\rI can now use pmap() from purrr to loop through the rows of this tibble. I have to be careful with my argument order, though, since I have the y variable first in the tibble but it is the second argument in my function.\nallplots2 = pmap(resp_expl, ~scatter_fun(x = .y, y = .x) )\rI’m not going to print them here, but one minor downside of this approach is that only the first variable name is carried along to the output list. If I wanted to save these plots separately and use both the response and explanatory variable in the file name I’d need to create the file names based on resp_expl. I won’t show this in the next section on saving plots, so here is a quick example of creating basic file names based on both variables.\nallplots2_names = pmap(resp_expl, ~ paste0(.x, \u0026quot;_\u0026quot;, .y, \u0026quot;.png\u0026quot;))\rallplots2_names[1:2]\r## $elev\r## [1] \u0026quot;elev_slp.png\u0026quot;\r## ## $elev\r## [1] \u0026quot;elev_lat.png\u0026quot;\r\rSaving the plots\rOnce all the graphs are made we can look at them in R by printing the list or parts of the list as above. But if you want to peruse them at your leisure later or send them to a collaborator you’ll want to save them outside of R.\nThis next section is dedicated to exploring some of the ways you can do this.\n\rSaving all plots to one PDF\rIf you want to save every plot as a separate page in a PDF, you can do so with the pdf() function. The code below shows an example of how this works. First, a graphics device to save the plots into is created and given a name via pdf(). Then all the plots are put into that device. Finally, the device is turned off with dev.off(). The last step is important, as you can’t open the file until the device is turned off.\nThis is a pretty coarse way to save everything, but it allows you to easily page through all the plots. I’ve used this method when I had many exploratory plots for a single response variable that I wanted to share with collaborators.\n\rIn this example code I save the file, which I name all_scatterplots.pdf, into my current working directory.\rpdf(\u0026quot;all_scatterplots.pdf\u0026quot;)\rall_plots\r## $elev\r## $elev$slp\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $elev$lat\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $elev$long\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $elev$nt\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## ## $resp\r## $resp$slp\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $resp$lat\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $resp$long\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $resp$nt\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## ## $grad\r## $grad$slp\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $grad$lat\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $grad$long\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $grad$nt\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rdev.off()\r## png ## 2\r\rSaving groups of plots together\rAnother option is to save each group of plots in a separate document. This might make sense in a case like this where there are a set of plots for each response variable and we might want a separate file for each set. To save each sub-list separately we’ll need to loop through all_plots and save the plots for each response variable into a separate file. The list names can be used in the file names to keep the output organized. The functions in purrr that start with i are special functions that loop through a list and the names of that list simultaneously. This is useful here where we want to use the list names to identify the output files while we save them. The walk() function is part of the map family, to be used when you want a function for its side effect instead of for a return value. Saving plots is a classic example of when we want walk() instead of map(). Combining the i and the walk gives us the iwalk() function. In the formula interface, .x refers to the list elements and .y refers to the names of the list. You can see I create the plot file names using the list name combined with “scatter plots.pdf”, using _ as the separator. The code below makes three files, one for each response variable, with four plots each. The files are named “elev_scatter plots.pdf”, “resp_scatter plots.pdf”, and “grad_scatter plots.pdf”.\niwalk(all_plots, ~ {\rpdf(paste0(.y, \u0026quot;_scatterplots.pdf\u0026quot;))\rprint(.x)\rdev.off()\r})\r## $slp\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $lat\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $long\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $nt\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $slp\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $lat\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $long\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $nt\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $slp\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $lat\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $long\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## ## $nt\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r\rSaving all plots separately\rAll plots can be saved separately instead of combined in a single document. This might be necessary if you want to insert the plots into some larger document later. We’ll want to use the names of both the outer and inner lists to appropriately identify each plot we save. I decided to do this by looping through the all_plots list and the names of the list via imap() to make the file names in a separate step. This time I’m going to save these as PNG files so use .png at the end of the file name. The result is a list of lists, so I flatten this into a single list via flatten(). If I were to use flatten() earlier in the process I’d lose the names of the outer list. This process of combining names prior to flattening should be simplified once the proposed flatten_names() function is added to purrr.\nplotnames = imap(all_plots, ~ paste0(.y, \u0026quot;_\u0026quot;, names(.x), \u0026quot;.png\u0026quot;)) %\u0026gt;%\rflatten()\rOnce the file names are created I can loop through all the file names and plots simultaneously with walk2() and save things via ggsave(). The height and width of each output file can be set as needed in ggsave(). You can see I flattened the nested list of plots into a single list to use in walk2().\nwalk2(plotnames,\rflatten(all_plots),\r~ ggsave(\rfilename = .x,\rplot = .y,\rheight = 7,\rwidth = 7\r))\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r\rCombining plots\rAnother way to get a set of plots together is to combine them into one plot. How useful this is will depend on how many plots you have per set. This option is a lot like faceting, except we didn’t reshape our data set to allow the use faceting. I like the package cowplot function plot_grid() for quickly combining multiple plots into one. A list of plots can be passed via the plotlist argument.\n\rHere’s what that looks like for the first response variable, elev.\rcowplot::plot_grid(plotlist = all_plots[[1]])\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rWe can use a loop to combine the plots for each response variable sub-list. The result could then be saved using any of the approaches shown above. If you have many subplots per combined plot you likely will want to save the plots at a larger size so the individual plots can be clearly seen.\nresponse_plots = map(all_plots, ~ cowplot::plot_grid(plotlist = .x))\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rresponse_plots\r## $elev\r## ## $resp\r## ## $grad\rTo learn more on functional programming in R you can look at Jenny Bryan blog post on functional programming here\n\r\r","date":1636588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636638829,"objectID":"6f9685b49d96e4900b3cea58885f099c","permalink":"https://statisticalinference.netlify.app/post/automating-exploratory-plots-with-ggplot2-and-purrr/","publishdate":"2021-11-11T00:00:00Z","relpermalink":"/post/automating-exploratory-plots-with-ggplot2-and-purrr/","section":"post","summary":"When you have a lot of variables and need to make multiple exploratory plots it’s usually worthwhile to automate the process in R instead of manually copying and pasting code for every plot, this thing happen to me alot but when I read Hadley Wickham Advance R book on functional programming, I knew that what I was doing was wrong.","tags":["Purrr"],"title":"Automating exploratory plots with ggplot2 and purrr","type":"post"},{"authors":[],"categories":["R path"],"content":"The goal of the here package is to enable easy file referencing in a project-oriented workflow. In contrast to using setwd(), which is fragile and dependent on the way you organize your files, here uses the top-level directory of a project to easily build paths to files. When I got started with R, I learned to put setwd() and rm(list=ls()) at the beginning of my scripts. It made sense to me but it seems like it got rid of any leftovers in the environment and set up the working directory, so I could use relative paths. That seems to be a good practice, right? The idea definitely is, but setwd() and rm(list=ls()) are problematic. rm() doesn\u0026rsquo;t give you a clean R session; it won\u0026rsquo;t for instance, detach packages. setwd(), meanwhile, is completely dependent on the way you organize your files because the absolute file path in your computer is different from the file path in my computer; so if you have to share that R script with anyone the file path needs to be modified before that script will run in another system. In 2017, Jenny Bryan gave a talk at the R studio conference on Project-oriented workflow. A couple of slides, in particular, set off a bit of controversy: If the first line of your R script is:\nsetwd(\u0026lsquo;C:/Users/Jenny/Path/that/only/I/have\u0026rsquo;)\nI will come into your office and SET YOUR COMPUTER ON FIRE\nIf the first line of your R script is:\nrm(list=ls())\r I will come into your office and SET YOUR COMPUTER ON FIRE\nI agree with this opinion from Jenny Bryan! Here I explain why these habits can be harmful and may be indicative of an awkward workflow.\n  I strongly recommend that we should use the R studio projects. These set up a local working directory in a fresh R session, which makes it much easier for someone else to open and run your code.\n  Use here() from here package to write file paths.\n  When we want to write a file into a specific folder we can also use the here package to write the absolute path to save the file.\nwrite.csv(mtcars,here::here('post/mtcars.csv'))\r We can also use the here package to read file from a specific location within a particular directory of the project we are working on.\nread.csv(here::here('post/mtcars.csv'))\r In conclusion projects can not only help solve the problems of setwd() but also rm(list=ls()). The need for setwd() is automatically eliminated by using projects because the default directory will be wherever the project is located. You can learn more by reading the vignette of the here package in R by clicking here or Jenny Bryan tutorial by clicking here\nh1.title {\rfont-size: 12px;\rcolor: Dark;\rtext-align: justify;\r}\r\rbody{\rtext-align: justify}\r","date":1634083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634132363,"objectID":"9775aad7668c2c3b051acfefcdd1459c","permalink":"https://statisticalinference.netlify.app/post/2021-10-13-advice-on-setting-up-of-your-r-working-directory-to-maximize-effectiveness-and-reduce-frustration/","publishdate":"2021-10-13T00:00:00Z","relpermalink":"/post/2021-10-13-advice-on-setting-up-of-your-r-working-directory-to-maximize-effectiveness-and-reduce-frustration/","section":"post","summary":"The goal of the here package is to enable easy file referencing in a project-oriented workflow. In contrast to using setwd(), which is fragile and dependent on the way you organize your files, here uses the top-level directory of a project to easily build paths to files.","tags":["here"],"title":"Advice on setting up your working directory in R to maximize effectiveness and reduce frustration","type":"post"},{"authors":[],"categories":[],"content":"\r\rA 1/3-day workshop that takes your ggplot2 data visualizations to the next level. You’ll create beautiful, expressive plots that communicate effectively. You’ll learn highlighting, direct labelling, annotation, combining plots and more.\nMeet your instructor\rOluwafemi Oyedele\n\r","date":1633824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633880218,"objectID":"cc9fc8e156316f4e12db971e221a68a5","permalink":"https://statisticalinference.netlify.app/courses/data-visualization-with-ggplot2-bootcamp/","publishdate":"2021-10-10T00:00:00Z","relpermalink":"/courses/data-visualization-with-ggplot2-bootcamp/","section":"courses","summary":"A 1/3-day workshop that takes your ggplot2 data visualizations to the next level. You’ll create beautiful, expressive plots that communicate effectively. You’ll learn highlighting, direct labelling, annotation, combining plots and more.","tags":["ggplot2","R"],"title":"Data Visualization with ggplot2 Bootcamp","type":"courses"},{"authors":[],"categories":[],"content":"\r\rA 3-day workshop introducing the Tidyverse, basic modelling, and R markdown. In the R Bootcamp, you will learn how to read, transform, tidy, and visualize your data in R. You’ll also learn the basics of modelling in R and how to write reproducible documents with R Markdown.\nMeet your instructor\rOluwafemi Oyedele\nh1.title {\rfont-size: 12px;\rcolor: Dark;\rtext-align: centre;\r}\n\rbody{\rtext-align: justify}\r\r\r","date":1633824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633880002,"objectID":"277057d5efef5cdf9dfb88605cd8e6f1","permalink":"https://statisticalinference.netlify.app/courses/r-bootcamp/","publishdate":"2021-10-10T00:00:00Z","relpermalink":"/courses/r-bootcamp/","section":"courses","summary":"A 3-day workshop introducing the Tidyverse, basic modelling, and R markdown. In the R Bootcamp, you will learn how to read, transform, tidy, and visualize your data in R.","tags":["R"],"title":"R Bootcamp","type":"courses"},{"authors":[],"categories":[],"content":"\rbody{\rtext-align: justify}\r\r","date":1611187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633957990,"objectID":"eada2c327fae1109ea7180d1247afbdc","permalink":"https://statisticalinference.netlify.app/publication/journal-articles/","publishdate":"2021-01-21T00:00:00Z","relpermalink":"/publication/journal-articles/","section":"publication","summary":"The study highlights the evaluation of saturated hydraulic characteristics and its influence on some physical and chemical properties of soils developed on coastal plain sands of Obufa Esuk Orok in Calabar, Cross River State, Nigeria. Sixteen grids designed in an experimental plot measured 6 m x 6 m were used for field studies and sixteen (16) soil samples were collected in each of the grid using a soil auger for particle size analysis, bulk density, particle density, total porosity and saturated hydraulic conductivity. The samples were analyzed using standard laboratory procedures. The result showed that the soils were predominantly high in sand content with a mean value of 860.6 g kg-1 and low in silt and clay contents with mean values of 56.1 g kg-1 and 83.3 g kg-1 respectively. The soil texture was predominantly loamy sand. The saturated hydraulic conductivity showed rapid with a mean value of 36 cm min-1. Total porosity was high, a mean value of 52.4 %. Bulk density was low, a mean value of 1.21 Mg m-3 while Particle density was moderate, mean value of 2.55 Mgm-3.  The soil pH showed very strongly acid milieu (mean pH in water = 5.1). Organic carbon and Total nitrogen were low with mean values of 1.1 % and 0.09 % respectively. Available phosphorus was high with a mean value of 36.66 mg kg-1. The exchangeable acidity and exchangeable bases were generally low with mean values of 2.54, 0.59, 0.08 and 0.053 cmolc/kg for calcium, magnesium, potassium and sodium and 0.261and 0.416 cmolc/kg for aluminum and hydrogen. The correlation coefficient (r) between the saturated hydraulic conductivity and texture showed that there was a positive relationship between saturated hydraulic conductivity and sand, silt and clay (correlation coefficient of r = 0.0013, 0.062 and 0.119) at p≤0.05 indicating good relationship. There was also a positive linear relationship between the saturated hydraulic conductivity and bulk density, particle density and total porosity (correlation coefficient values of r = 0.224, 0.03 and 0.107) at p≤0.05 respectively. Despite the positive relationship existed in their correlation, cultural practices such as minimum, zero, mulch tillage and other conservational practices should be adopted to help maintain the rapid condition of the saturated hydraulic conductivity to avoid restriction of water movement and other soluble nutrients in the soil.","tags":null,"title":"Evaluation of Saturated Hydraulic Characteristics and its Influence on Some Physical and Chemical Properties of Soils Developed on Coastal Plain Sands of Obufa Esuk Orok in Calabar, Cross River State, Nigeria","type":"publication"},{"authors":["Oluwafemi Oyedele"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://statisticalinference.netlify.app/publication/conference-paper/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"","tags":null,"title":"Conference paper","type":"publication"},{"authors":null,"categories":null,"content":"Contribution of cocoa litter to nutrients recycling in low-shade Southwestern Nigeria. This was my research work in which I conducted at the International Institute of Tropical Agriculture (IITA), Ibadan as a research fellow that led to the award of my MSc degree in the department of Soil Resources Management at the University of Ibadan, Nigeria.\n\rbody{\rtext-align: justify}\r\r","date":1573516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573516800,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://statisticalinference.netlify.app/project/internal-project/","publishdate":"2019-11-12T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"Contribution of cocoa litter to nutrients recycling in low-shade Southwestern Nigeria. This was my research work in which I conducted at the International Institute of Tropical Agriculture (IITA), Ibadan as a research fellow that led to the award of my MSc degree in the department of Soil Resources Management at the University of Ibadan, Nigeria.","tags":["Nutrient Cycling"],"title":"Internal Project","type":"project"},{"authors":["Oluwafemi Oyedele"],"categories":null,"content":"\rbody{\rtext-align: justify}\r\r","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://statisticalinference.netlify.app/publication/journal-article/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Infiltration of water into the soil is an important physical process affecting the fate of water under field conditions, especially the amount of subsurface recharge and surface runoff and hence the hazard  of  soil  erosion.  The  study  was  conducted  to  evaluate  the  infiltration  models  of  soils developed  on  coastal  plain  sands  and  to  select  a  suitable  models  as  a  basis  to  improve  the management of the soil. A total of 16 infiltration runs were made with the double ring infiltrometer. For the purpose of getting best fitting model, the results obtained from various infiltration models were compared with observed field data. The parameters considered for best fitting of model were correlation coefficient  and  coefficient  of  variability  (CV).  Model-predicted cumulative  infiltration consistently  deviated  from  field-measured data,that  is,  the  models  under-predicted  cumulative infiltration  by several  orders  of  magnitude  for  Kostiakov,  Green  Ampt  and  Philip model  but  the model  over  predicted  cumulative  infiltration  for Horton  model.  The  results  of  the  soil  samples analysed revealed that the mean values of 707.50, 208.13 and 84.38 gkg-1 for sand, silt and clay with the textural class of sandy loam. The bulk density, particle density and total porosity had mean values  of  1.84  gcm-3,  2.44  gcm-3  and 22.56%.  However,  there  was  a  fairly  good  agreement between mean-measured cumulative infiltration (7.30 cm/hr, CV = 32.19%); Philips (1.93 cm/hr, CV = 42.49%); Kostiakov  (0.13  cm/hr, CV = 30.77%); Horton (64.49  cm/hr,  CV = 22.39%)  and Green Ampt model (42.04 cm/hr, CV = 0.57%) respectively. The data however showed that the correlation coefficient for Kostiakov (1.00) was best fitting in predicting the field measured data and this was closely followed by Green Ampt (0.88); while Philip’s model and Horton model showed a negative correlation (r = -0.88 and r = -0.82) with the field measured data. Conservation measures involving  mulching,  cover  cropping  and  afforestation  are  recommended  to  improve  the  soil structure and infiltration capacity.","tags":null,"title":"Evaluation of Infiltration Characteristics of Soils Developed on Coastal Plain Sands in Calabar Municipality Local Government Area, Cross River State, Nigeria.","type":"publication"},{"authors":null,"categories":null,"content":"\rbody{\rtext-align: justify}\r\r","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://statisticalinference.netlify.app/publication/preprint/","publishdate":"2019-04-07T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"\rbody{\rtext-align: justify}\r\r","tags":null,"title":"Preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://statisticalinference.netlify.app/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://statisticalinference.netlify.app/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"\u0026hellip;","tags":null,"title":"Privacy Policy","type":"page"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://statisticalinference.netlify.app/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://statisticalinference.netlify.app/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]